algorithm: PPO

stop:
    episode_reward_mean: 20000
    training_iteration: 1000
    timesteps_total: 1000000

config:
    env: godot
    env_config:
        framerate: null
        action_repeat: null
        show_window: false

    framework: torch
    
    train_batch_size: 1024
    sgd_minibatch_size: 128
    lr: 0.0003
    entropy_coeff: 0.001
    entropy_coeff_schedule: null
    vf_clip_param: 1.0
    clip_param: 0.2
    lambda: 0.95
    gamma: 0.95
    num_sgd_iter: 16

    model:
        fcnet_hiddens: [256, 256]
        use_lstm: false
        lstm_cell_size: 32
        framestack: 4

    batch_mode: truncate_episodes
    num_workers: 8
    num_envs_per_worker: 8
    rollout_fragment_length: 16
    num_gpus: 1
    no_done_at_end: False
    soft_horizon: False