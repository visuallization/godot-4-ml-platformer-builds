algorithm: PPO

stop:
    episode_reward_mean: 20000
    training_iteration: 20000
    timesteps_total: 6000000

config:
    env: godot
    env_config:
        framerate: null
        action_repeat: null
        show_window: false

    framework: torch
    
    train_batch_size: 512
    sgd_minibatch_size: 64
    lr: 0.0003
    entropy_coeff: 0.005
    entropy_coeff_schedule: null
    vf_clip_param: 10000.0
    clip_param: 0.2
    lambda: 0.95
    gamma: 0.99
    num_sgd_iter: 2

    model:
        fcnet_hiddens: [64, 64]
        use_lstm: false
        lstm_cell_size: 32
        framestack: 4

    batch_mode: truncate_episodes
    num_workers: 8
    num_envs_per_worker: 2
    rollout_fragment_length: 32
    num_gpus: 1
    no_done_at_end: False
    soft_horizon: False